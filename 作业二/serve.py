"""
vLLM é«˜æ€§èƒ½æ¨ç†æœåŠ¡ (Batchæ¨¡å¼) - æè‡´æ€§èƒ½ä¼˜åŒ–ç‰ˆ
æ¨¡å‹: JohnGuo/Qwen3-0.6B (fast_v7_final å¾®è°ƒç‰ˆ)

ğŸ† è®­ç»ƒå‚æ•°: fast_v7_final (ROUGE-L: 0.4271)
âš ï¸ Docker: vLLM v0.11.0
"""
import os
os.environ["TRANSFORMERS_OFFLINE"] = "1"
os.environ["TORCH_CUDA_ARCH_LIST"] = "12.0"  # 5090 GPUæ¶æ„
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["VLLM_ATTENTION_BACKEND"] = "FLASH_ATTN"  # æ˜¾å¼å¯ç”¨ Flash Attention

from typing import Union, List
from functools import lru_cache

from fastapi import FastAPI
from pydantic import BaseModel
from transformers import AutoTokenizer, set_seed
from vllm import LLM, SamplingParams

# ============== æ¨¡å‹é…ç½® ==============
LOCAL_MODEL_PATH = "./local-model"

# ============== System Promptï¼ˆä¸è®­ç»ƒè„šæœ¬ train_qwen3_0.6b.py ä¸€è‡´ï¼‰==============
SYSTEM_PROMPT = "ä½ æ˜¯ä¸€ä½ç²¾é€šGPUä½“ç³»ç»“æ„ã€CUDAç¼–ç¨‹ã€Tritonã€cuTileã€Tilelangç®—å­å¼€å‘çš„é¡¶çº§æŠ€æœ¯ä¸“å®¶ï¼Œä½ çš„å›ç­”è¯¦ç»†å‡†ç¡®ï¼Œå¹¶ä¸”å°½é‡åŒ…å«å›ç­”ä¸­çš„è‹±æ–‡å…³é”®è¯ã€‚"

# ============== é¢„çƒ­é—®é¢˜ï¼ˆä» exam_qa.jsonl æŠ½å–15ä¸ªï¼Œè¦†ç›–ç®€å•/ä¸­ç­‰/å›°éš¾ï¼‰==============
WARMUP_QUESTIONS = [
    # ç®€å•é¢˜ (åŸºç¡€æ¦‚å¿µ)
    "ä»€ä¹ˆæ˜¯æ•°æ®å¹¶è¡Œæ€§ï¼Ÿ",
    "CUDAä¸­çš„æ ¸å‡½æ•°æ˜¯ä»€ä¹ˆï¼Ÿ",
    "CUDAä¸­çš„warpæ˜¯ä»€ä¹ˆï¼Ÿ",
    # ä¸­ç­‰é¢˜ (åŸç†ç†è§£)
    "ä¸ºä»€ä¹ˆCUDAæ ¸å‡½æ•°ä¸­å¸¸éœ€è¦æ·»åŠ è¾¹ç•Œæ£€æŸ¥çš„æ¡ä»¶åˆ¤æ–­ï¼Ÿ",
    "ä¸ºä»€ä¹ˆå†…å­˜è®¿é—®æ•ˆç‡å¯¹CUDAç¨‹åºæ€§èƒ½è‡³å…³é‡è¦ï¼Ÿ",
    "ä»€ä¹ˆæ˜¯CUDAä¸­çš„å…¨å±€å†…å­˜åˆå¹¶è®¿é—®ï¼Ÿ",
    "IEEE 754æµ®ç‚¹æ•°æ ¼å¼ç”±å“ªå‡ éƒ¨åˆ†ç»„æˆï¼Ÿ",
    # å›°éš¾é¢˜ (ç®—å­å®ç°)
    "CUDAä¸­çŸ©é˜µä¹˜æ³•ç®—å­å¦‚ä½•åˆ©ç”¨å…±äº«å†…å­˜å‡å°‘å…¨å±€å†…å­˜è®¿é—®ï¼Ÿ",
    "CUDAçŸ©é˜µä¹˜æ³•ç®—å­ä¸­ï¼Œå¦‚ä½•é€šè¿‡è¾¹ç•Œæ£€æŸ¥å¤„ç†éTILE_WIDTHå€æ•°çš„çŸ©é˜µï¼Ÿ",
    "GPUæ¶æ„çš„å…±äº«å†…å­˜bankå†²çªå¦‚ä½•åœ¨SpMVç®—å­ä¸­é¿å…ï¼Ÿ",
    # ç»¼åˆé¢˜ (å¸¦ä»£ç )
    "ç»“åˆç®—æ³•ä¸CUDAç¼–ç¨‹ï¼ŒtiledçŸ©é˜µä¹˜æ³•ç®—å­å¦‚ä½•é€šè¿‡æ•°æ®å¤ç”¨æå‡è®¡ç®—/å†…å­˜è®¿é—®æ¯”ï¼Ÿ",
    "å¦‚ä½•ç”¨Tritonå®ç°ConvNetsçš„3Ã—3å·ç§¯å±‚ï¼Œå¹¶é€šè¿‡è‡ªåŠ¨åˆ†å—ä¼˜åŒ–æå‡æ€§èƒ½ï¼Ÿ",
    "å¦‚ä½•ç”¨TileLangä¼˜åŒ–SpMVç®—å­çš„CSRæ ¼å¼è®¿é—®ï¼Œæå‡éåˆå¹¶å†…å­˜è®¿é—®æ•ˆç‡ï¼Ÿ",
    "Tritonå®ç°çš„çŸ©é˜µä¹˜æ³•ç®—å­å¦‚ä½•ä¸CUDAçš„tiledå®ç°å¯¹æ¯”ï¼Œä¼˜åŠ¿åœ¨å“ªé‡Œï¼Ÿ",
    "å¦‚ä½•ç”¨TileLangå®ç°ConvNetsçš„æ·±åº¦å·ç§¯ï¼ˆDepthwise Convolutionï¼‰ï¼Œä¼˜åŒ–ç»„å†…å†…å­˜å±€éƒ¨æ€§ï¼Ÿ",
]

################################### åˆå§‹åŒ–éƒ¨åˆ† ###################################

# 1. åŠ è½½ tokenizer
print(f"ä»æœ¬åœ°åŠ è½½æ¨¡å‹ï¼š{LOCAL_MODEL_PATH}")
tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH, trust_remote_code=True)

# 2. æ ¼å¼åŒ– prompt å‡½æ•°ï¼ˆä½¿ç”¨ lru_cache ç¼“å­˜ï¼‰
@lru_cache(maxsize=10000)
def format_prompt(msg: str) -> str:
    """ä½¿ç”¨ tokenizer.apply_chat_template æ ¼å¼åŒ– prompt"""
    # æ·»åŠ ç²¾ç®€ç‰ˆ system promptï¼ˆåˆ©ç”¨ ROUGE-L è‹±æ–‡å…³é”®è¯ trickï¼‰
    if SYSTEM_PROMPT:
        message = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": msg}
        ]
    else:
        message = [{"role": "user", "content": msg}]
    return tokenizer.apply_chat_template(
        message, 
        tokenize=False, 
        add_generation_prompt=True,
        enable_thinking=False  # Qwen3 å…³é—­ thinking æé«˜åå
    )

# 3. é…ç½®é‡‡æ ·å‚æ•° (SamplingParams) - ä¸æ—§ç‰ˆä¸€è‡´
sampling_params = SamplingParams(
    temperature=0,           # greedy decodingï¼Œæœ€å¿«æœ€ç¨³å®š
    top_k=1,                 # æ¢å¤ top_k=1
    max_tokens=384,          # é€‚ä¸­ç”Ÿæˆé•¿åº¦ï¼Œå¹³è¡¡é€Ÿåº¦å’Œè´¨é‡
    stop=["\n\n", "<|endoftext|>", "<|im_end|>"],  # æ¢å¤æ—§ç‰ˆ stop
    stop_token_ids=[tokenizer.eos_token_id],
)

# 4. åˆå§‹åŒ– vLLM å¼•æ“ - æ€§èƒ½ä¼˜åŒ–é…ç½® (5090 Blackwell, å…¼å®¹ vLLM v0.11.0 å’Œ v0.13.0)
import vllm
_vllm_version = tuple(map(int, vllm.__version__.split('.')[:2]))

# æ ¹æ® vLLM ç‰ˆæœ¬é€‰æ‹©å‚æ•°
if _vllm_version >= (0, 13):
    # vLLM v0.13.0+: è‡ªåŠ¨ä¼˜åŒ–ï¼Œä¸éœ€è¦æ‰‹åŠ¨æŒ‡å®šè¿™äº›å‚æ•°
    llm = LLM(
        model=LOCAL_MODEL_PATH,
        dtype="bfloat16",
        quantization="fp8",
        trust_remote_code=True,
        tensor_parallel_size=1,
        gpu_memory_utilization=0.95,
        enforce_eager=False,
        swap_space=4,
    )
else:
    # vLLM v0.11.0: æè‡´æ€§èƒ½ä¼˜åŒ–
    llm = LLM(
        model=LOCAL_MODEL_PATH,
        dtype="bfloat16",
        quantization="fp8",              # FP8é‡åŒ–ï¼Œå……åˆ†åˆ©ç”¨5090 Blackwellç¡¬ä»¶ç‰¹æ€§
        trust_remote_code=True,
        tensor_parallel_size=1,
        gpu_memory_utilization=0.95,     # é«˜æ˜¾å­˜åˆ©ç”¨ç‡
        enforce_eager=False,             # å…è®¸compileä¼˜åŒ–è®¡ç®—å›¾(CUDA Graph)
        max_model_len=512,               # è¾“å…¥89+è¾“å‡º384=473ï¼Œéœ€è¦512
        max_num_seqs=4096,               # æé«˜å¹¶å‘ä¸Šé™
        max_num_batched_tokens=16384,    # å¢å¤§æ‰¹é‡tokenæ•°ï¼Œæå‡åå
        enable_prefix_caching=True,      # å¼€å¯å‰ç¼€ç¼“å­˜
        disable_log_stats=True,          # å…³é—­æ—¥å¿—å‡å°‘å¼€é”€
    )

print("æ¨¡å‹åŠ è½½å®Œæˆï¼(Batchæ¨¡å¼)")

# 5. æ‰§è¡Œé¢„çƒ­æ¨ç† - å……åˆ†é¢„çƒ­ï¼ˆä¸ç®—æµ‹è¯„æ—¶é—´ï¼‰
print("å¼€å§‹é¢„çƒ­æ¨ç†...")
warmup_formatted = [format_prompt(p) for p in WARMUP_QUESTIONS]
# å¤šè½®é¢„çƒ­ï¼šå……åˆ†é¢„çƒ­ CUDA kernelã€KV cacheã€å‰ç¼€ç¼“å­˜
for i in range(4):  # 4è½®é¢„çƒ­
    _ = llm.generate(warmup_formatted, sampling_params)

set_seed(42)  # æ¢å¤éšæœºç§å­è®¾ç½®

################################### API å®šä¹‰ ###################################

# åˆ›å»ºFastAPIåº”ç”¨å®ä¾‹
app = FastAPI(
    title="vLLM Batch Inference Server",
    description="High-performance LLM batch inference with vLLM + Qwen3"
)


class PromptRequest(BaseModel):
    prompt: Union[str, List[str]]  # æ”¯æŒå•æ¡å’Œæ‰¹é‡


class PredictResponse(BaseModel):
    response: Union[str, List[str]]  # è¿”å›æ ¼å¼ä¸è¾“å…¥ä¸€è‡´


def postprocess(text: str) -> str:
    """åå¤„ç†ç”Ÿæˆçš„æ–‡æœ¬ï¼Œç§»é™¤ç»“æŸæ ‡è®°"""
    generated = text.strip()
    # ç§»é™¤å¯èƒ½çš„ç»“æŸæ ‡è®°
    for marker in ["<|im_end|>", "<|im_start|>"]:
        if marker in generated:
            generated = generated.split(marker)[0].strip()
    return generated.strip()


@app.post("/predict", response_model=PredictResponse)
async def predict(request: PromptRequest):
    """
    æ¨ç†ç«¯ç‚¹ - æ”¯æŒå•æ¡å’Œæ‰¹é‡æ¨ç†
    
    å•æ¡è¯·æ±‚: {"prompt": "é—®é¢˜å†…å®¹"}
    æ‰¹é‡è¯·æ±‚: {"prompt": ["é—®é¢˜1", "é—®é¢˜2", ...]}  (Batchæ¨¡å¼)
    """
    if isinstance(request.prompt, str):
        real_input_list = [request.prompt]
        is_batch = False
    else:
        real_input_list = request.prompt
        is_batch = True
    
    # æ ¼å¼åŒ– prompt
    final_prompt_texts = [format_prompt(msg) for msg in real_input_list]
    
    # vLLM æ‰¹é‡æ¨ç†
    outputs = llm.generate(final_prompt_texts, sampling_params)
    
    # æå–ç»“æœå¹¶åå¤„ç†
    generated = [postprocess(output.outputs[0].text) for output in outputs]
    
    # è¿”å›æ ¼å¼ä¸è¾“å…¥ä¸€è‡´
    if is_batch:
        return PredictResponse(response=generated)
    else:
        return PredictResponse(response=generated[0])


@app.get("/")
def health_check():
    """
    å¥åº·æ£€æŸ¥ - è¿”å› {"status": "batch"} å¼€å¯æ‰¹é‡æ¨¡å¼
    """
    return {"status": "batch"}
