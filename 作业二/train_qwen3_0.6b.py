#!/usr/bin/env python3
"""
Qwen3-0.6B LoRA å¾®è°ƒè®­ç»ƒè„šæœ¬
å‚è€ƒ Megatron è®­ç»ƒå‚æ•°é…ç½®
ä½¿ç”¨ tokenizer.apply_chat_template æ ¼å¼åŒ–ï¼ˆä¸æ¨ç†æœåŠ¡ä¸€è‡´ï¼‰
ä½¿ç”¨ ROUGE-L ä½œä¸ºå‡†ç¡®ç‡è¯„ä¼°æŒ‡æ ‡
"""

# ============== é…ç½®åŒº ==============
CUDA_DEVICE = 0

import os
os.environ['CUDA_VISIBLE_DEVICES'] = str(CUDA_DEVICE)

import json
import random
import torch
import numpy as np
from pathlib import Path
from datetime import datetime
from datasets import Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, EarlyStoppingCallback
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer, SFTConfig
from rouge_score import rouge_scorer
import jieba

# ============== è·¯å¾„é…ç½® ==============
BASE_DIR = Path('/mnt/vepfs01/output/guojunhao/GPU')

# æ¨¡å‹é…ç½®
MODEL_NAME = "Qwen3-0.6B"
MODEL_DIR = BASE_DIR / 'models'
MODEL_PATH = MODEL_DIR / MODEL_NAME

# ModelScope æ¨¡å‹ IDï¼ˆç”¨äºä¸‹è½½ï¼‰
MODELSCOPE_MODEL_ID = "Qwen/Qwen3-0.6B"

# æ•°æ®é›†è·¯å¾„
TRAIN_DATA = [
    BASE_DIR / 'datasets' / 'processed_dataset1.jsonl',   # 8140æ¡ (question/answeræ ¼å¼)
  ##  BASE_DIR / 'datasets' / 'pmpp_qa_with_exam.jsonl',    # 2229æ¡ (instruction/outputæ ¼å¼)
    BASE_DIR / 'datasets' / 'pmpp_qa_v2.jsonl',           # 3099æ¡ (instruction/outputæ ¼å¼)
]  # åˆè®¡ 13468 æ¡è®­ç»ƒæ•°æ®
EVAL_DATA = BASE_DIR / 'datasets' / 'exam_qa.jsonl'       # æµ‹è¯•é›† (193æ¡)

# ============== è®­ç»ƒè¶…å‚ Profileï¼ˆæ¨èç”¨è¿™ä¸ªåˆ‡æ¢ä¸€ç»„å‚æ•°ï¼‰==============
# baseline: åŸå§‹é…ç½®ï¼ˆåä¿å®ˆï¼Œå­¦ä¹ ç‡è¿‡ä½ï¼‰
# quality_v1: ä¿å®ˆé…ç½®ï¼ˆå­¦ä¹ ç‡ 1e-4ï¼Œ3 epochsï¼‰
# fast_v1: ğŸ† å½“å‰æœ€ä½³é…ç½® (ROUGE-L: 0.3768, eval_loss: 1.0181)
# fast_v2: åŸºäº fast_v1 è°ƒä¼˜ï¼Œå¢åŠ è®­ç»ƒè½®æ¬¡åˆ° 2 epochs
# fast_v3: åŸºäº fast_v1 è°ƒä¼˜ï¼Œå¢å¤§ LoRA rank åˆ° 32
# fast_v4: åŸºäº fast_v1 è°ƒä¼˜ï¼Œå¢åŠ åºåˆ—é•¿åº¦åˆ° 512
# æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡ TRAIN_PROFILE æŒ‡å®šï¼Œé»˜è®¤ä½¿ç”¨ fast_v1ï¼ˆå½“å‰æœ€ä½³ï¼‰
PROFILE = os.environ.get("TRAIN_PROFILE", "fast_v1")

PROFILES = {
    "baseline": {
        "MAX_SEQ_LENGTH": 512,
        "LORA_R": 64,
        "LORA_ALPHA": 128,
        "LORA_DROPOUT": 0.10,
        "BATCH_SIZE": 4,
        "GRADIENT_ACCUMULATION_STEPS": 4,  # æœ‰æ•ˆ batch=16
        "LEARNING_RATE": 1e-5,
        "MIN_LEARNING_RATE": 1e-6,
        "NUM_EPOCHS": 3,
        "WARMUP_RATIO": 0.10,
        "WEIGHT_DECAY": 0.01,
        "EARLY_STOPPING_PATIENCE": 1,
    },
    "quality_v1": {
        # ä¿å®ˆé…ç½®ï¼Œ3 epochsï¼Œå­¦ä¹ ç‡ 1e-4
        "MAX_SEQ_LENGTH": 512,
        "LORA_R": 32,
        "LORA_ALPHA": 64,
        "LORA_DROPOUT": 0.05,
        "BATCH_SIZE": 4,
        "GRADIENT_ACCUMULATION_STEPS": 8,  # æœ‰æ•ˆ batch=32
        "LEARNING_RATE": 1e-4,
        "MIN_LEARNING_RATE": 1e-5,
        "NUM_EPOCHS": 3,
        "WARMUP_RATIO": 0.05,
        "WEIGHT_DECAY": 0.0,
        "EARLY_STOPPING_PATIENCE": 2,
    },
    "fast_v1": {
        # ğŸ† å½“å‰æœ€ä½³é…ç½® (ROUGE-L: 0.3768, eval_loss: 1.0181)
        # é«˜å­¦ä¹ ç‡ 2e-4 + è½»é‡ LoRA + 1 epoch
        "MAX_SEQ_LENGTH": 384,
        "LORA_R": 16,
        "LORA_ALPHA": 32,
        "LORA_DROPOUT": 0.05,
        "BATCH_SIZE": 4,
        "GRADIENT_ACCUMULATION_STEPS": 4,  # æœ‰æ•ˆ batch=16
        "LEARNING_RATE": 2e-4,
        "MIN_LEARNING_RATE": 2e-5,
        "NUM_EPOCHS": 1,
        "WARMUP_RATIO": 0.03,
        "WEIGHT_DECAY": 0.0,
        "EARLY_STOPPING_PATIENCE": 1,
    },
    "fast_v2": {
        # åŸºäº fast_v1ï¼Œå¢åŠ è®­ç»ƒè½®æ¬¡åˆ° 2 epochsï¼ˆçœ‹æ˜¯å¦èƒ½è¿›ä¸€æ­¥æå‡ï¼‰
        "MAX_SEQ_LENGTH": 384,
        "LORA_R": 16,
        "LORA_ALPHA": 32,
        "LORA_DROPOUT": 0.05,
        "BATCH_SIZE": 4,
        "GRADIENT_ACCUMULATION_STEPS": 4,  # æœ‰æ•ˆ batch=16
        "LEARNING_RATE": 2e-4,
        "MIN_LEARNING_RATE": 2e-5,
        "NUM_EPOCHS": 2,
        "WARMUP_RATIO": 0.03,
        "WEIGHT_DECAY": 0.0,
        "EARLY_STOPPING_PATIENCE": 2,
    },
    "fast_v3": {
        # åŸºäº fast_v1ï¼Œå¢å¤§ LoRA rank åˆ° 32ï¼ˆæ›´å¤šå¯è®­ç»ƒå‚æ•°ï¼‰
        "MAX_SEQ_LENGTH": 384,
        "LORA_R": 32,
        "LORA_ALPHA": 64,
        "LORA_DROPOUT": 0.05,
        "BATCH_SIZE": 4,
        "GRADIENT_ACCUMULATION_STEPS": 4,  # æœ‰æ•ˆ batch=16
        "LEARNING_RATE": 2e-4,
        "MIN_LEARNING_RATE": 2e-5,
        "NUM_EPOCHS": 1,
        "WARMUP_RATIO": 0.03,
        "WEIGHT_DECAY": 0.0,
        "EARLY_STOPPING_PATIENCE": 1,
    },
    "fast_v4": {
        # åŸºäº fast_v1ï¼Œå¢åŠ åºåˆ—é•¿åº¦åˆ° 512ï¼ˆæ•æ‰æ›´é•¿ä¸Šä¸‹æ–‡ï¼‰
        "MAX_SEQ_LENGTH": 512,
        "LORA_R": 16,
        "LORA_ALPHA": 32,
        "LORA_DROPOUT": 0.05,
        "BATCH_SIZE": 4,
        "GRADIENT_ACCUMULATION_STEPS": 4,  # æœ‰æ•ˆ batch=16
        "LEARNING_RATE": 2e-4,
        "MIN_LEARNING_RATE": 2e-5,
        "NUM_EPOCHS": 1,
        "WARMUP_RATIO": 0.03,
        "WEIGHT_DECAY": 0.0,
        "EARLY_STOPPING_PATIENCE": 1,
    },
    "fast_v5": {
        # ç»¼åˆä¼˜åŒ–ï¼šseq=512 + 2 epochs + æ›´ä½å­¦ä¹ ç‡ + æ­£åˆ™åŒ–
        # ç›®æ ‡ï¼šåœ¨è¯„ä¼°é›†ä¸Šå–å¾—æ›´å¥½çš„æ³›åŒ–æ•ˆæœ
        # ç»“æœï¼šROUGE-L 0.3864 ğŸ†
        "MAX_SEQ_LENGTH": 512,
        "LORA_R": 16,
        "LORA_ALPHA": 32,
        "LORA_DROPOUT": 0.10,       # å¢åŠ  dropout é˜²æ­¢è¿‡æ‹Ÿåˆ
        "BATCH_SIZE": 4,
        "GRADIENT_ACCUMULATION_STEPS": 4,  # æœ‰æ•ˆ batch=16
        "LEARNING_RATE": 1.5e-4,    # ç¨ä½å­¦ä¹ ç‡ï¼Œæ›´ç¨³å®šæ”¶æ•›
        "MIN_LEARNING_RATE": 1.5e-5,
        "NUM_EPOCHS": 2,            # 2 epochs
        "WARMUP_RATIO": 0.05,
        "WEIGHT_DECAY": 0.01,       # æ·»åŠ æƒé‡è¡°å‡
        "EARLY_STOPPING_PATIENCE": 2,
    },
    "fast_v6": {
        # åŸºäº fast_v5ï¼Œæ›´ä½å­¦ä¹ ç‡ + 3 epochs
        "MAX_SEQ_LENGTH": 512,
        "LORA_R": 16,
        "LORA_ALPHA": 32,
        "LORA_DROPOUT": 0.10,
        "BATCH_SIZE": 4,
        "GRADIENT_ACCUMULATION_STEPS": 4,  # æœ‰æ•ˆ batch=16
        "LEARNING_RATE": 1e-4,      # æ›´ä½å­¦ä¹ ç‡
        "MIN_LEARNING_RATE": 1e-5,
        "NUM_EPOCHS": 3,            # 3 epochs
        "WARMUP_RATIO": 0.05,
        "WEIGHT_DECAY": 0.01,
        "EARLY_STOPPING_PATIENCE": 2,
    },
    "fast_v7": {
        # åŸºäº fast_v5ï¼Œå¢å¤§ LoRA rank åˆ° 32
        "MAX_SEQ_LENGTH": 512,
        "LORA_R": 32,               # å¢å¤§ rank
        "LORA_ALPHA": 64,
        "LORA_DROPOUT": 0.10,
        "BATCH_SIZE": 4,
        "GRADIENT_ACCUMULATION_STEPS": 4,  # æœ‰æ•ˆ batch=16
        "LEARNING_RATE": 1.5e-4,
        "MIN_LEARNING_RATE": 1.5e-5,
        "NUM_EPOCHS": 2,
        "WARMUP_RATIO": 0.05,
        "WEIGHT_DECAY": 0.01,
        "EARLY_STOPPING_PATIENCE": 2,
    },
    "fast_v8": {
        # åŸºäº fast_v5ï¼Œæ›´å¤§æ‰¹æ¬¡ + æ›´é«˜å­¦ä¹ ç‡
        "MAX_SEQ_LENGTH": 512,
        "LORA_R": 16,
        "LORA_ALPHA": 32,
        "LORA_DROPOUT": 0.10,
        "BATCH_SIZE": 4,
        "GRADIENT_ACCUMULATION_STEPS": 8,  # æœ‰æ•ˆ batch=32
        "LEARNING_RATE": 2e-4,      # å¤§æ‰¹æ¬¡å¯ç”¨æ›´é«˜å­¦ä¹ ç‡
        "MIN_LEARNING_RATE": 2e-5,
        "NUM_EPOCHS": 2,
        "WARMUP_RATIO": 0.05,
        "WEIGHT_DECAY": 0.01,
        "EARLY_STOPPING_PATIENCE": 2,
    },
    "fast_v7_final": {
        # ğŸ† æœ€ä¼˜é…ç½® fast_v7 + ä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒï¼ˆåŒ…å«æµ‹è¯•é›†ï¼‰
        # ROUGE-L: 0.3566ï¼ˆæµ‹è¯•é›†ï¼‰
        # ç”¨äºæœ€ç»ˆéƒ¨ç½²
        "MAX_SEQ_LENGTH": 512,
        "LORA_R": 32,               # æœ€ä¼˜ï¼šå¤§ rank
        "LORA_ALPHA": 64,
        "LORA_DROPOUT": 0.10,
        "BATCH_SIZE": 4,
        "GRADIENT_ACCUMULATION_STEPS": 4,  # æœ‰æ•ˆ batch=16
        "LEARNING_RATE": 1.5e-4,
        "MIN_LEARNING_RATE": 1.5e-5,
        "NUM_EPOCHS": 2,
        "WARMUP_RATIO": 0.05,
        "WEIGHT_DECAY": 0.01,
        "EARLY_STOPPING_PATIENCE": 2,
        "USE_ALL_DATA": True,       # ğŸ”¥ åŒ…å«æµ‹è¯•é›†è®­ç»ƒ
    },
}

if PROFILE not in PROFILES:
    raise ValueError(f"Unknown PROFILE: {PROFILE}. Available: {list(PROFILES.keys())}")

_P = PROFILES[PROFILE]

# ============== æ¨¡å‹é…ç½®ï¼ˆå‚è€ƒ Megatron å‚æ•°ï¼‰==============
MAX_SEQ_LENGTH = _P["MAX_SEQ_LENGTH"]

# ============== LoRA é…ç½® ==============
LORA_R = _P["LORA_R"]
LORA_ALPHA = _P["LORA_ALPHA"]
LORA_DROPOUT = _P["LORA_DROPOUT"]
TARGET_MODULES = [
    "q_proj", "k_proj", "v_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj"
]

# ============== è®­ç»ƒé…ç½®ï¼ˆå‚è€ƒ Megatron å‚æ•°ï¼‰==============
BATCH_SIZE = _P["BATCH_SIZE"]
GRADIENT_ACCUMULATION_STEPS = _P["GRADIENT_ACCUMULATION_STEPS"]
LEARNING_RATE = _P["LEARNING_RATE"]
MIN_LEARNING_RATE = _P["MIN_LEARNING_RATE"]
NUM_EPOCHS = _P["NUM_EPOCHS"]
WARMUP_RATIO = _P["WARMUP_RATIO"]

# ============== è¯„ä¼°é…ç½® ==============
EVAL_STRATEGY = "epoch"       # æ¯ä¸ª epoch è¯„ä¼°ä¸€æ¬¡
SAVE_STRATEGY = "epoch"       # æ¯ä¸ª epoch ä¿å­˜ä¸€æ¬¡

# ============== é˜²è¿‡æ‹Ÿåˆé…ç½® ==============
WEIGHT_DECAY = _P["WEIGHT_DECAY"]
EARLY_STOPPING_PATIENCE = _P["EARLY_STOPPING_PATIENCE"]

def _fmt_float(x: float) -> str:
    # ä¸ç”¨ç§‘å­¦è®¡æ•°æ³•ï¼Œè¾“å‡ºå®Œæ•´å°æ•°ï¼ˆå»æ‰å°¾éƒ¨æ— æ„ä¹‰çš„ 0ï¼‰
    s = f"{x:.10f}".rstrip("0").rstrip(".")
    return s if s else "0"

# ============== è¾“å‡ºç›®å½•ï¼ˆåŒ…å«å…³é”®å‚æ•°ï¼Œä¸ä½¿ç”¨çœç•¥/ç§‘å­¦è®¡æ•°æ³•ï¼‰==============
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
effective_bs = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS
lr_str = _fmt_float(LEARNING_RATE)
wd_str = _fmt_float(WEIGHT_DECAY)
drop_str = _fmt_float(LORA_DROPOUT)
# æ ¼å¼ç¤ºä¾‹:
# qwen3_0.6b_profquality_v1_seq512_r32_a64_d0.05_ep3_lr0.0001_wd0_bs32_20260104_123456
OUTPUT_DIR = (
    BASE_DIR
    / "outputs"
    / (
        f"qwen3_0.6b_prof{PROFILE}"
        f"_seq{MAX_SEQ_LENGTH}"
        f"_r{LORA_R}_a{LORA_ALPHA}_d{drop_str}"
        f"_ep{NUM_EPOCHS}"
        f"_lr{lr_str}_wd{wd_str}"
        f"_bs{effective_bs}"
        f"_{timestamp}"
    )
)

# ============== ROUGE-L è¯„ä¼°é…ç½® ==============
ROUGE_EVAL_SAMPLES = None        # ROUGE-Lè¯„ä¼°æ ·æœ¬æ•°ï¼ˆNone=å…¨éƒ¨ï¼‰

# ============== æ¨¡å‹ä¸Šä¼ é…ç½® ==============
UPLOAD_MODEL = True            # è®­ç»ƒå®Œæˆåæ˜¯å¦ä¸Šä¼ æ¨¡å‹
MODELSCOPE_TOKEN = "ms-21a8ae09-100b-4187-ad36-33b377db0cf0"
MODELSCOPE_REPO_ID = "JohnGuo/Qwen3-0.6B"

# ============== System Promptï¼ˆä¸ serve.py æ¨ç†æœåŠ¡ä¸€è‡´ï¼‰==============
# å¼ºè°ƒä¸“ä¸šæ€§ + è‹±æ–‡å…³é”®è¯ï¼ˆåˆ©ç”¨ ROUGE-L å¯¹è‹±æ–‡çš„æ³¨æ„åŠ› trickï¼‰
SYSTEM_PROMPT = "ä½ æ˜¯ä¸€ä½ç²¾é€šGPUä½“ç³»ç»“æ„ã€CUDAç¼–ç¨‹ã€Tritonã€cuTileã€Tilelangç®—å­å¼€å‘çš„é¡¶çº§æŠ€æœ¯ä¸“å®¶ï¼Œä½ çš„å›ç­”è¯¦ç»†å‡†ç¡®ï¼Œå¹¶ä¸”å°½é‡åŒ…å«å›ç­”ä¸­çš„è‹±æ–‡å…³é”®è¯ã€‚"


# å…¨å±€ tokenizer å¼•ç”¨ï¼Œç”¨äºæ ¼å¼åŒ–å‡½æ•°
_tokenizer = None


def download_model():
    """
    ä» ModelScope ä¸‹è½½æ¨¡å‹åˆ° models ç›®å½•
    """
    if MODEL_PATH.exists() and (MODEL_PATH / 'config.json').exists():
        print(f"æ¨¡å‹å·²å­˜åœ¨: {MODEL_PATH}")
        return True
    
    print(f"\n{'='*70}")
    print(f"  ä¸‹è½½æ¨¡å‹: {MODELSCOPE_MODEL_ID}")
    print(f"  ç›®æ ‡è·¯å¾„: {MODEL_PATH}")
    print(f"{'='*70}\n")
    
    try:
        from modelscope import snapshot_download
        
        MODEL_DIR.mkdir(parents=True, exist_ok=True)
        
        model_dir = snapshot_download(
            model_id=MODELSCOPE_MODEL_ID,
            cache_dir=str(MODEL_DIR),
            local_dir=str(MODEL_PATH),
        )
        
        print(f"\næ¨¡å‹ä¸‹è½½å®Œæˆ: {model_dir}")
        return True
        
    except ImportError:
        print("[æç¤º] æœªå®‰è£… modelscopeï¼Œå°è¯•ä½¿ç”¨ huggingface...")
        
        try:
            from huggingface_hub import snapshot_download
            
            MODEL_DIR.mkdir(parents=True, exist_ok=True)
            
            model_dir = snapshot_download(
                repo_id=f"Qwen/{MODEL_NAME}",
                local_dir=str(MODEL_PATH),
            )
            
            print(f"\næ¨¡å‹ä¸‹è½½å®Œæˆ: {model_dir}")
            return True
            
        except Exception as e:
            print(f"[é”™è¯¯] æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
            return False
    
    except Exception as e:
        print(f"[é”™è¯¯] æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
        return False


def format_prompt_with_template(instruction: str, output: str) -> str:
    """
    ä½¿ç”¨ tokenizer.apply_chat_template æ ¼å¼åŒ– promptï¼ˆä¸æ¨ç†æœåŠ¡ä¸€è‡´ï¼‰
    """
    # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆä¸ serve.py ä¿æŒä¸€è‡´ï¼Œä¸ä½¿ç”¨ system promptï¼‰
    if SYSTEM_PROMPT:
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": instruction},
            {"role": "assistant", "content": output}
        ]
    else:
        messages = [
            {"role": "user", "content": instruction},
            {"role": "assistant", "content": output}
        ]
    
    formatted = _tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=False,
    )
    
    return formatted


def format_prompt_for_generation(instruction: str) -> str:
    """
    æ ¼å¼åŒ–ç”¨äºç”Ÿæˆçš„ promptï¼ˆä¸åŒ…å« assistant å›å¤ï¼‰
    ä¸ serve.py æ¨ç†æœåŠ¡ä¸€è‡´
    """
    # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆä¸ serve.py ä¿æŒä¸€è‡´ï¼Œä¸ä½¿ç”¨ system promptï¼‰
    if SYSTEM_PROMPT:
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": instruction},
        ]
    else:
        messages = [
            {"role": "user", "content": instruction},
        ]
    
    formatted = _tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=False,  # Qwen3 å…³é—­ thinkingï¼ˆä¸ serve.py ä¸€è‡´ï¼‰
    )
    
    return formatted


def get_accuracy(predictions, ground_truths):
    """
    ä½¿ç”¨ rouge_scorer å’Œ jieba è®¡ç®— ROUGE-L å‡†ç¡®ç‡
    """
    try:
        diff = len(ground_truths) - len(predictions)
        if diff > 0:
            predictions.extend([""] * diff)

        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=False)
        
        scores = []
        for pred, ref in zip(predictions, ground_truths):

            pred_tokens = " ".join(jieba.lcut(pred))
            ref_tokens = " ".join(jieba.lcut(ref))
            
            if not pred_tokens.strip() or not ref_tokens.strip():
                scores.append(0.0)
                continue

            score = scorer.score(ref_tokens, pred_tokens)
            scores.append(score['rougeL'].fmeasure)
        
        return sum(scores) / len(scores) if scores else 0
        
    except Exception as e:
        print(f"Evaluation error: {e}")
        return 0


def load_dataset_from_jsonl(path, shuffle: bool = True, for_eval: bool = False):
    """
    ä»JSONLæ–‡ä»¶åŠ è½½æ•°æ®é›†ï¼Œæ”¯æŒå•æ–‡ä»¶æˆ–å¤šæ–‡ä»¶åˆ—è¡¨
    """
    # æ”¯æŒå•æ–‡ä»¶æˆ–å¤šæ–‡ä»¶
    if isinstance(path, list):
        paths = path
    else:
        paths = [path]
    
    raw_items = []
    for p in paths:
        print(f"åŠ è½½æ•°æ®é›†: {p}")
        with open(p, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    raw_items.append(json.loads(line.strip()))
    
    if shuffle:
        random.seed(42)
        random.shuffle(raw_items)
    
    data = []
    for item in raw_items:
        # æ”¯æŒä¸¤ç§æ ¼å¼:
        # 1. æ—§æ ¼å¼: {"instruction": é—®é¢˜, "input": "", "output": ç­”æ¡ˆ}
        # 2. æ–°æ ¼å¼: {"instruction": ç³»ç»Ÿæç¤º, "question": é—®é¢˜, "answer": ç­”æ¡ˆ}
        if 'question' in item:
            instruction = item.get('question', '')
            output = item.get('answer', '')
        else:
            instruction = item.get('instruction', '')
            output = item.get('output', '')
        
        text = format_prompt_with_template(instruction, output)
        
        if for_eval:
            data.append({
                'text': text,
                'instruction': instruction,
                'reference': output,
            })
        else:
            data.append({'text': text})

    dataset = Dataset.from_list(data)
    print(f"  åŠ è½½ {len(dataset)} æ¡æ•°æ®")

    return dataset


def evaluate_with_rouge_l(model, tokenizer, eval_raw_data: list, max_samples: int = None):
    """
    ä½¿ç”¨ROUGE-Lè¯„ä¼°æ¨¡å‹å‡†ç¡®ç‡
    """
    model.eval()
    
    if max_samples:
        eval_samples = eval_raw_data[:max_samples]
    else:
        eval_samples = eval_raw_data
    
    predictions = []
    ground_truths = []
    
    print(f"\nä½¿ç”¨ ROUGE-L è¯„ä¼°å‡†ç¡®ç‡ ({len(eval_samples)} ä¸ªæ ·æœ¬)...")
    
    for idx, sample in enumerate(eval_samples):
        instruction = sample['instruction']
        reference = sample['reference']
        
        input_text = format_prompt_for_generation(instruction)
        
        inputs = tokenizer(input_text, return_tensors='pt').to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=384,       # ä¸ serve.py ä¸€è‡´
                temperature=0,            # greedy decodingï¼ˆä¸ serve.py ä¸€è‡´ï¼‰
                top_k=1,
                do_sample=False,          # ç¦ç”¨é‡‡æ ·ï¼Œä½¿ç”¨ greedy
                pad_token_id=tokenizer.eos_token_id,
            )
        
        generated = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        
        # è°ƒè¯•ï¼šæ˜¾ç¤ºå‰3ä¸ªæ ·æœ¬çš„ç”Ÿæˆç»“æœï¼ˆå®Œæ•´è¾“å‡ºï¼‰
        if idx < 3:
            print(f"\n[æ ·æœ¬ {idx+1}]")
            print(f"  é—®é¢˜: {instruction}")
            print(f"  ç”Ÿæˆ: {generated}")
            print(f"  å‚è€ƒç­”æ¡ˆ: {reference}")
        
        predictions.append(generated)
        ground_truths.append(reference)
        
        if (idx + 1) % 20 == 0:
            # è®¡ç®—å½“å‰å·²è¯„ä¼°æ ·æœ¬çš„å¹³å‡å‡†ç¡®ç‡
            current_accuracy = get_accuracy(predictions.copy(), ground_truths.copy())
            print(f"  å·²è¯„ä¼° {idx + 1}/{len(eval_samples)}ï¼Œå½“å‰å¹³å‡ROUGE-L: {current_accuracy:.4f}")
    
    # ä½¿ç”¨ get_accuracy è®¡ç®—æœ€ç»ˆå‡†ç¡®ç‡
    avg_rouge_l = get_accuracy(predictions, ground_truths)
    print(f"\næœ€ç»ˆ ROUGE-L å‡†ç¡®ç‡: {avg_rouge_l:.4f}")
    
    # âš ï¸ è°ƒè¯•ï¼šæµ‹è¯•å¦‚æœç›´æ¥ç”¨é—®é¢˜ä½œä¸ºç­”æ¡ˆä¼šå¾—åˆ°å¤šå°‘åˆ†
    test_questions_as_answers = [sample['instruction'] for sample in eval_samples]
    baseline_score = get_accuracy(test_questions_as_answers, ground_truths)
    print(f"\n[è°ƒè¯•] å¦‚æœç›´æ¥ç”¨é—®é¢˜ä½œä¸ºç­”æ¡ˆçš„ ROUGE-L: {baseline_score:.4f}")
    print(f"[è°ƒè¯•] æ¨¡å‹å®é™…è¡¨ç° vs åŸºçº¿: {avg_rouge_l:.4f} vs {baseline_score:.4f}")
    
    return avg_rouge_l


def load_eval_raw_data(path: Path) -> list:
    """
    åŠ è½½è¯„ä¼°æ•°æ®çš„åŸå§‹æ ¼å¼ï¼ˆç”¨äºROUGE-Lè¯„ä¼°ï¼‰
    æ”¯æŒä¸¤ç§æ ¼å¼
    """
    raw_items = []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                item = json.loads(line.strip())
                # æ”¯æŒä¸¤ç§æ ¼å¼
                if 'question' in item:
                    instruction = item.get('question', '')
                    reference = item.get('answer', '')
                else:
                    instruction = item.get('instruction', '')
                    reference = item.get('output', '')
                raw_items.append({
                    'instruction': instruction,
                    'reference': reference,
                })
    return raw_items


def main():
    global _tokenizer
    
    print("="*70)
    print("  Qwen3-0.6B LoRA å¾®è°ƒè®­ç»ƒ")
    print("  å‚è€ƒ Megatron è®­ç»ƒå‚æ•°é…ç½®")
    print("  ä½¿ç”¨ tokenizer.apply_chat_template æ ¼å¼åŒ–")
    print("  è¯„ä¼°æŒ‡æ ‡: eval_loss + ROUGE-L å‡†ç¡®ç‡")
    print("="*70)

    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"\nGPU: {gpu_name} ({gpu_mem:.1f}GB)")
    else:
        print("\n[é”™è¯¯] æœªæ£€æµ‹åˆ°GPU")
        return

    # ä¸‹è½½æ¨¡å‹
    print("\n[æ­¥éª¤ 1] æ£€æŸ¥/ä¸‹è½½æ¨¡å‹...")
    if not download_model():
        print("[é”™è¯¯] æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹")
        return

    # æ‰“å°é…ç½®
    print(f"\né…ç½®ï¼ˆå‚è€ƒ Megatron å‚æ•°ï¼‰:")
    print(f"  æ¨¡å‹: {MODEL_NAME}")
    if isinstance(TRAIN_DATA, list):
        print(f"  è®­ç»ƒæ•°æ®: {len(TRAIN_DATA)} ä¸ªæ–‡ä»¶")
        for p in TRAIN_DATA:
            print(f"    - {p.name}")
    else:
        print(f"  è®­ç»ƒæ•°æ®: {TRAIN_DATA.name}")
    print(f"  è¯„ä¼°æ•°æ®: {EVAL_DATA.name}")
    print(f"  åºåˆ—é•¿åº¦: {MAX_SEQ_LENGTH}")
    print(f"  LoRA: r={LORA_R}, alpha={LORA_ALPHA}")
    print(f"  Batch: {BATCH_SIZE} x {GRADIENT_ACCUMULATION_STEPS} = {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}")
    print(f"  å­¦ä¹ ç‡: {LEARNING_RATE} -> {MIN_LEARNING_RATE}")
    print(f"  è®­ç»ƒè½®æ¬¡: {NUM_EPOCHS}")
    print(f"  è¯„ä¼°ç­–ç•¥: {EVAL_STRATEGY}")
    print(f"  ROUGE-Lè¯„ä¼°æ ·æœ¬æ•°: {ROUGE_EVAL_SAMPLES}")
    print(f"  ç²¾åº¦: bf16")

    # åŠ è½½ tokenizer
    print(f"\n[æ­¥éª¤ 2] åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
    _tokenizer = AutoTokenizer.from_pretrained(str(MODEL_PATH), trust_remote_code=True)
    _tokenizer.pad_token = _tokenizer.eos_token
    _tokenizer.padding_side = 'right'
    
    # åŠ è½½è®­ç»ƒæ•°æ®é›†
    use_all_data = _P.get("USE_ALL_DATA", False)
    if use_all_data:
        # ğŸ”¥ ä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒï¼ˆåŒ…å«æµ‹è¯•é›†ï¼‰- ç”¨äºæœ€ç»ˆéƒ¨ç½²æ¨¡å‹
        print(f"  âš ï¸ ä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒï¼ˆåŒ…å«æµ‹è¯•é›†ï¼‰")
        train_data_paths = TRAIN_DATA if isinstance(TRAIN_DATA, list) else [TRAIN_DATA]
        train_data_paths = train_data_paths + [EVAL_DATA]
        train_dataset = load_dataset_from_jsonl(train_data_paths, shuffle=True, for_eval=False)
        eval_dataset = train_dataset  # è¯„ä¼°é›†ä½¿ç”¨è®­ç»ƒé›†ï¼ˆåªç”¨äºç›‘æ§ï¼‰
        eval_raw_data = load_eval_raw_data(EVAL_DATA)  # ROUGE-L ä»ç”¨æµ‹è¯•é›†
    else:
        train_dataset = load_dataset_from_jsonl(TRAIN_DATA, shuffle=True, for_eval=False)
        # åŠ è½½è¯„ä¼°æ•°æ®é›†ï¼ˆå›ºå®šæµ‹è¯„é›†ï¼Œä¸æ‰“ä¹±ï¼‰
        eval_dataset = load_dataset_from_jsonl(EVAL_DATA, shuffle=False, for_eval=False)
        # åŠ è½½è¯„ä¼°æ•°æ®çš„åŸå§‹æ ¼å¼ï¼ˆç”¨äºROUGE-Lè¯„ä¼°ï¼‰
        eval_raw_data = load_eval_raw_data(EVAL_DATA)
    
    print(f"  Promptæ ¼å¼: tokenizer.apply_chat_templateï¼ˆä¸æ¨ç†æœåŠ¡ä¸€è‡´ï¼‰")

    # åŠ è½½æ¨¡å‹
    print(f"\n[æ­¥éª¤ 3] åŠ è½½æ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(
        str(MODEL_PATH),
        torch_dtype=torch.bfloat16,
        device_map={'': 0},
        trust_remote_code=True,
    )
    model.gradient_checkpointing_enable()
    
    # é…ç½® LoRA
    print(f"\n[æ­¥éª¤ 4] é…ç½® LoRA...")
    lora_config = LoraConfig(
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        lora_dropout=LORA_DROPOUT,
        target_modules=TARGET_MODULES,
        bias='none',
        task_type='CAUSAL_LM',
    )
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    # è®­ç»ƒ
    print(f"\n[æ­¥éª¤ 5] å¼€å§‹è®­ç»ƒ...")
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    sft_config = SFTConfig(
        output_dir=str(OUTPUT_DIR),
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        learning_rate=LEARNING_RATE,
        num_train_epochs=NUM_EPOCHS,
        warmup_ratio=WARMUP_RATIO,
        fp16=False,
        bf16=True,
        logging_steps=10,
        # è¯„ä¼°é…ç½®ï¼ˆæ¯ä¸ªepochè¯„ä¼°ä¸€æ¬¡ï¼‰
        eval_strategy=EVAL_STRATEGY,
        save_strategy=SAVE_STRATEGY,
        save_total_limit=3,
        load_best_model_at_end=True,
        metric_for_best_model='eval_loss',
        greater_is_better=False,
        weight_decay=WEIGHT_DECAY,
        lr_scheduler_type='cosine',
        # å…¶ä»–é…ç½®
        optim='adamw_torch',
        seed=42,
        report_to='none',
        dataloader_pin_memory=False,
        # SFT ç‰¹æœ‰é…ç½®
        dataset_text_field='text',
        max_length=MAX_SEQ_LENGTH,
    )

    trainer = SFTTrainer(
        model=model,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        processing_class=_tokenizer,
        args=sft_config,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=EARLY_STOPPING_PATIENCE)],
    )

    trainer.train()

    # ä¿å­˜ LoRA æƒé‡
    print(f"\n[æ­¥éª¤ 6] ä¿å­˜ LoRA æƒé‡åˆ°: {OUTPUT_DIR}")
    model.save_pretrained(OUTPUT_DIR)
    _tokenizer.save_pretrained(OUTPUT_DIR)
    
    # è·å–æœ€ç»ˆ eval_loss
    final_eval = trainer.evaluate()
    final_eval_loss = final_eval.get('eval_loss', None)
    print(f"\næœ€ç»ˆ eval_loss: {final_eval_loss:.4f}")

    # ä½¿ç”¨ ROUGE-L è¯„ä¼°å‡†ç¡®ç‡ï¼ˆåœ¨åˆå¹¶å‰ï¼Œä½¿ç”¨åŸå§‹ modelï¼‰
    print("\n" + "="*70)
    print("  ROUGE-L å‡†ç¡®ç‡è¯„ä¼°")
    print("="*70)
    
    final_rouge_l = evaluate_with_rouge_l(
        model, 
        _tokenizer, 
        eval_raw_data, 
        max_samples=ROUGE_EVAL_SAMPLES
    )
    
    # åˆå¹¶ LoRA åˆ°åŸºç¡€æ¨¡å‹ï¼Œç”Ÿæˆå®Œæ•´æ¨¡å‹
    print(f"\n[æ­¥éª¤ 7] åˆå¹¶ LoRA ç”Ÿæˆå®Œæ•´æ¨¡å‹...")
    merged_model_dir = OUTPUT_DIR / 'merged_model'
    merged_model_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆå¹¶æƒé‡
    merged_model = model.merge_and_unload()
    merged_model.save_pretrained(merged_model_dir)
    _tokenizer.save_pretrained(merged_model_dir)
    print(f"  å®Œæ•´æ¨¡å‹å·²ä¿å­˜åˆ°: {merged_model_dir}")
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    eval_results = {
        'model': MODEL_NAME,
        'eval_loss': final_eval_loss,
        'rouge_l': final_rouge_l,
        'rouge_l_samples': min(ROUGE_EVAL_SAMPLES, len(eval_raw_data)) if ROUGE_EVAL_SAMPLES else len(eval_raw_data),
        'train_samples': len(train_dataset),
        'eval_samples': len(eval_dataset),
        'config': {
            'max_seq_length': MAX_SEQ_LENGTH,
            'lora_r': LORA_R,
            'lora_alpha': LORA_ALPHA,
            'batch_size': BATCH_SIZE,
            'gradient_accumulation_steps': GRADIENT_ACCUMULATION_STEPS,
            'learning_rate': LEARNING_RATE,
            'num_epochs': NUM_EPOCHS,
            'eval_strategy': EVAL_STRATEGY,
        }
    }
    
    eval_results_path = OUTPUT_DIR / 'eval_results.json'
    with open(eval_results_path, 'w', encoding='utf-8') as f:
        json.dump(eval_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nè¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {eval_results_path}")

    print("\n" + "="*70)
    print("  è®­ç»ƒå®Œæˆ!")
    print("="*70)
    print(f"\nLoRA æƒé‡: {OUTPUT_DIR}")
    print(f"å®Œæ•´æ¨¡å‹: {OUTPUT_DIR / 'merged_model'}")
    print(f"eval_loss: {final_eval_loss:.4f}")
    print(f"ROUGE-L å‡†ç¡®ç‡: {final_rouge_l:.4f}")
    
    # ä¸Šä¼ æ¨¡å‹åˆ° ModelScope
    if UPLOAD_MODEL:
        upload_to_modelscope(OUTPUT_DIR / 'merged_model')


def upload_to_modelscope(model_dir: Path):
    """ä¸Šä¼ æ¨¡å‹åˆ° ModelScope"""
    import subprocess
    import shutil
    
    print("\n" + "="*70)
    print("  ä¸Šä¼ æ¨¡å‹åˆ° ModelScope")
    print("="*70)
    print(f"æ¨¡å‹ç›®å½•: {model_dir}")
    print(f"ç›®æ ‡ä»“åº“: {MODELSCOPE_REPO_ID}")
    
    if not model_dir.exists():
        print(f"é”™è¯¯ï¼šæ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
        return False
    
    # åˆ—å‡ºæ¨¡å‹æ–‡ä»¶
    print("\næ¨¡å‹æ–‡ä»¶:")
    total_size = 0
    for f in model_dir.iterdir():
        if f.is_file():
            size = f.stat().st_size
            total_size += size
            print(f"  {f.name}: {size / 1024 / 1024:.2f} MB")
    print(f"  æ€»å¤§å°: {total_size / 1024 / 1024 / 1024:.2f} GB")
    
    # æ£€æŸ¥ Git LFS
    try:
        subprocess.run(['git', 'lfs', 'version'], check=True, capture_output=True)
        subprocess.run(['git', 'lfs', 'install'], capture_output=True)
    except:
        print("é”™è¯¯ï¼šGit LFS æœªå®‰è£…")
        return False
    
    # æ„å»º Git URL
    git_url = f"https://oauth2:{MODELSCOPE_TOKEN}@www.modelscope.cn/{MODELSCOPE_REPO_ID}.git"
    username, model_name = MODELSCOPE_REPO_ID.split('/', 1)
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    temp_dir = Path("/tmp/modelscope_upload_0.6b")
    if temp_dir.exists():
        shutil.rmtree(temp_dir)
    temp_dir.mkdir(parents=True, exist_ok=True)
    repo_dir = temp_dir / model_name
    
    try:
        print(f"\n[1/5] å…‹éš†ä»“åº“...")
        result = subprocess.run(['git', 'clone', git_url, str(repo_dir)], capture_output=True, text=True)
        if result.returncode != 0:
            print(f"  å…‹éš†å¤±è´¥: {result.stderr}")
            print(f"  è¯·å…ˆåœ¨ ModelScope åˆ›å»ºä»“åº“: {MODELSCOPE_REPO_ID}")
            return False
        print("  âœ“ ä»“åº“å…‹éš†æˆåŠŸ")
        
        print(f"\n[2/5] å¤åˆ¶æ¨¡å‹æ–‡ä»¶...")
        # åˆ é™¤æ—§æ–‡ä»¶
        for old_file in repo_dir.iterdir():
            if old_file.is_file() and old_file.suffix in ['.safetensors', '.bin', '.json', '.model', '.txt']:
                old_file.unlink()
        
        # å¤åˆ¶æ–°æ–‡ä»¶
        file_count = 0
        for file_path in model_dir.iterdir():
            if file_path.is_file():
                shutil.copy2(file_path, repo_dir / file_path.name)
                file_count += 1
                print(f"  å·²å¤åˆ¶: {file_path.name}")
        print(f"  âœ“ å·²å¤åˆ¶ {file_count} ä¸ªæ–‡ä»¶")
        
        print(f"\n[3/5] é…ç½® Git LFS...")
        import os
        os.chdir(repo_dir)
        
        for file_path in repo_dir.iterdir():
            if file_path.is_file() and file_path.stat().st_size > 10 * 1024 * 1024:
                subprocess.run(['git', 'lfs', 'track', file_path.name], capture_output=True)
        subprocess.run(['git', 'add', '.gitattributes'], capture_output=True)
        
        print(f"\n[4/5] æäº¤æ›´æ”¹...")
        subprocess.run(['git', 'config', 'user.name', username], capture_output=True)
        subprocess.run(['git', 'config', 'user.email', f'{username}@modelscope.cn'], capture_output=True)
        subprocess.run(['git', 'add', '.'], check=True)
        subprocess.run(['git', 'commit', '-m', 'Upload Qwen3-0.6B finetuned model'], capture_output=True)
        
        print(f"\n[5/5] æ¨é€åˆ° ModelScope...")
        result = subprocess.run(['git', 'push', '-u', 'origin', 'master'], capture_output=True, text=True, timeout=1800)
        
        if result.returncode == 0:
            print("  âœ“ ä¸Šä¼ æˆåŠŸï¼")
            print(f"\næ¨¡å‹åœ°å€: https://modelscope.cn/models/{MODELSCOPE_REPO_ID}")
            return True
        else:
            # å°è¯• main åˆ†æ”¯
            result = subprocess.run(['git', 'push', '-u', 'origin', 'main'], capture_output=True, text=True, timeout=1800)
            if result.returncode == 0:
                print("  âœ“ ä¸Šä¼ æˆåŠŸï¼")
                print(f"\næ¨¡å‹åœ°å€: https://modelscope.cn/models/{MODELSCOPE_REPO_ID}")
                return True
            print(f"  æ¨é€å¤±è´¥: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        return False
    finally:
        if temp_dir.exists():
            shutil.rmtree(temp_dir, ignore_errors=True)


if __name__ == '__main__':
    main()
